{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1428159,"sourceType":"datasetVersion","datasetId":836401}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# | NLP | LLM | VectorDB | Weaviate |\n\n## Natural Language Processing (NLP) and Large Language Models (LLM) with Vector Database Weaviate\n\n![Learning](https://t3.ftcdn.net/jpg/06/14/01/52/360_F_614015247_EWZHvC6AAOsaIOepakhyJvMqUu5tpLfY.jpg)\n\n\n# <b>1 <span style='color:#78D118'>|</span> Overview</b>\n\n In this notebook, we will use Weaviate as our vector database. We will then write the embedding vectors out to Weaviate and query for similar documents. Weaviate provides customization options, such as to incorporate Product Quantization or not (refer [here](https://weaviate.io/developers/weaviate/concepts/vector-index#hnsw-with-product-quantizationpq)). \n \n[Zilliz](https://zilliz.com/) has an enterprise offering for Weaviate.\n\n<img src=\"https://mms.businesswire.com/media/20220824005057/en/1550928/5/Logo-_Colorful.jpg\" alt=\"Learning\" width=\"50%\">\n\n\n## Library pre-requisites\n\n- weaviate-client\n    - pip install below\n- Spark connector jar file\n    - **IMPORTANT!!** Since we will be interacting with Spark by writing a Spark dataframe out to Pinecone, we need a Spark Connector.\n    - You need to attach a Spark-Pinecone connector `s3://pinecone-jars/0.2.1/spark-pinecone-uberjar.jar` in the cluster you are using. Refer to this [documentation](https://docs.pinecone.io/docs/databricks#setting-up-a-spark-cluster) if you need more information. \n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### Setup\n","metadata":{}},{"cell_type":"code","source":"!pip install weaviate-client==3.19.1","metadata":{"execution":{"iopub.status.busy":"2023-12-29T10:23:45.658103Z","iopub.execute_input":"2023-12-29T10:23:45.658449Z","iopub.status.idle":"2023-12-29T10:23:58.716382Z","shell.execute_reply.started":"2023-12-29T10:23:45.658425Z","shell.execute_reply":"2023-12-29T10:23:58.715229Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Collecting weaviate-client==3.19.1\n  Obtaining dependency information for weaviate-client==3.19.1 from https://files.pythonhosted.org/packages/d6/3e/daa4e3fdd5dd3499ab12c262c21f3a28a9b868f73341703b791fa86e1b88/weaviate_client-3.19.1-py3-none-any.whl.metadata\n  Downloading weaviate_client-3.19.1-py3-none-any.whl.metadata (3.3 kB)\nCollecting requests<2.29.0,>=2.28.0 (from weaviate-client==3.19.1)\n  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting validators<=0.21.0,>=0.18.2 (from weaviate-client==3.19.1)\n  Obtaining dependency information for validators<=0.21.0,>=0.18.2 from https://files.pythonhosted.org/packages/ad/50/18dbf2ac594234ee6249bfe3425fa424c18eeb96f29dcd47f199ed6c51bc/validators-0.21.0-py3-none-any.whl.metadata\n  Downloading validators-0.21.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: tqdm<5.0.0,>=4.59.0 in /opt/conda/lib/python3.10/site-packages (from weaviate-client==3.19.1) (4.66.1)\nCollecting authlib>=1.1.0 (from weaviate-client==3.19.1)\n  Obtaining dependency information for authlib>=1.1.0 from https://files.pythonhosted.org/packages/25/65/b78eb948b71ab232d08b30c38a2e3b69e6e50c6e166863a0068c877155b9/Authlib-1.3.0-py2.py3-none-any.whl.metadata\n  Downloading Authlib-1.3.0-py2.py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: cryptography in /opt/conda/lib/python3.10/site-packages (from authlib>=1.1.0->weaviate-client==3.19.1) (41.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<2.29.0,>=2.28.0->weaviate-client==3.19.1) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<2.29.0,>=2.28.0->weaviate-client==3.19.1) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<2.29.0,>=2.28.0->weaviate-client==3.19.1) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<2.29.0,>=2.28.0->weaviate-client==3.19.1) (2023.11.17)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography->authlib>=1.1.0->weaviate-client==3.19.1) (1.15.1)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography->authlib>=1.1.0->weaviate-client==3.19.1) (2.21)\nDownloading weaviate_client-3.19.1-py3-none-any.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.6/99.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading Authlib-1.3.0-py2.py3-none-any.whl (223 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.7/223.7 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading validators-0.21.0-py3-none-any.whl (27 kB)\nInstalling collected packages: validators, requests, authlib, weaviate-client\n  Attempting uninstall: requests\n    Found existing installation: requests 2.31.0\n    Uninstalling requests-2.31.0:\n      Successfully uninstalled requests-2.31.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nbeatrix-jupyterlab 2023.814.150030 requires jupyter-server~=1.16, but you have jupyter-server 2.12.1 which is incompatible.\nbeatrix-jupyterlab 2023.814.150030 requires jupyterlab~=3.4, but you have jupyterlab 4.0.5 which is incompatible.\ngcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.12.2 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-pubsub 2.18.3 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.51.1 which is incompatible.\nkfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nspacy 3.7.2 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, but you have pydantic 1.8.1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.9.0 which is incompatible.\ntensorflowjs 4.14.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\nweasel 0.3.4 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, but you have pydantic 1.8.1 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed authlib-1.3.0 requests-2.28.2 validators-0.21.0 weaviate-client-3.19.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install sparkmagic\n!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2023-12-29T10:23:58.718341Z","iopub.execute_input":"2023-12-29T10:23:58.718653Z","iopub.status.idle":"2023-12-29T10:28:04.872963Z","shell.execute_reply.started":"2023-12-29T10:23:58.718623Z","shell.execute_reply":"2023-12-29T10:28:04.871956Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Collecting sparkmagic\n  Downloading sparkmagic-0.21.0.tar.gz (45 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting hdijupyterutils>=0.6 (from sparkmagic)\n  Downloading hdijupyterutils-0.21.0.tar.gz (5.1 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting autovizwidget>=0.6 (from sparkmagic)\n  Downloading autovizwidget-0.21.0.tar.gz (9.0 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: ipython>=4.0.2 in /opt/conda/lib/python3.10/site-packages (from sparkmagic) (8.14.0)\nCollecting pandas<2.0.0,>=0.17.1 (from sparkmagic)\n  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sparkmagic) (1.24.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from sparkmagic) (2.28.2)\nRequirement already satisfied: ipykernel>=4.2.2 in /opt/conda/lib/python3.10/site-packages (from sparkmagic) (6.25.1)\nRequirement already satisfied: ipywidgets>5.0.0 in /opt/conda/lib/python3.10/site-packages (from sparkmagic) (7.7.1)\nRequirement already satisfied: notebook>=4.2 in /opt/conda/lib/python3.10/site-packages (from sparkmagic) (6.5.5)\nRequirement already satisfied: tornado>=4 in /opt/conda/lib/python3.10/site-packages (from sparkmagic) (6.3.3)\nCollecting requests_kerberos>=0.8.0 (from sparkmagic)\n  Downloading requests_kerberos-0.14.0-py2.py3-none-any.whl (11 kB)\nRequirement already satisfied: nest_asyncio>1.5.5 in /opt/conda/lib/python3.10/site-packages (from sparkmagic) (1.5.6)\nRequirement already satisfied: plotly>=3 in /opt/conda/lib/python3.10/site-packages (from autovizwidget>=0.6->sparkmagic) (5.16.1)\nCollecting jupyter>=1 (from hdijupyterutils>=0.6->sparkmagic)\n  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\nRequirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.2.2->sparkmagic) (0.1.4)\nRequirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.2.2->sparkmagic) (1.6.7.post1)\nRequirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.2.2->sparkmagic) (7.4.9)\nRequirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.2.2->sparkmagic) (5.3.1)\nRequirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.2.2->sparkmagic) (0.1.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.2.2->sparkmagic) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.2.2->sparkmagic) (5.9.3)\nRequirement already satisfied: pyzmq>=20 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.2.2->sparkmagic) (24.0.1)\nRequirement already satisfied: traitlets>=5.4.0 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.2.2->sparkmagic) (5.9.0)\nRequirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.2->sparkmagic) (0.2.0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.2->sparkmagic) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.2->sparkmagic) (0.19.0)\nRequirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.2->sparkmagic) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.2->sparkmagic) (3.0.39)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.2->sparkmagic) (2.16.1)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.2->sparkmagic) (0.6.2)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.2->sparkmagic) (4.8.0)\nRequirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets>5.0.0->sparkmagic) (0.2.0)\nRequirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets>5.0.0->sparkmagic) (3.6.6)\nRequirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets>5.0.0->sparkmagic) (3.0.8)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.2->sparkmagic) (3.1.2)\nRequirement already satisfied: argon2-cffi in /opt/conda/lib/python3.10/site-packages (from notebook>=4.2->sparkmagic) (21.3.0)\nRequirement already satisfied: nbformat in /opt/conda/lib/python3.10/site-packages (from notebook>=4.2->sparkmagic) (5.9.2)\nRequirement already satisfied: nbconvert>=5 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.2->sparkmagic) (6.4.5)\nRequirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.2->sparkmagic) (1.8.2)\nRequirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.2->sparkmagic) (0.17.1)\nRequirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from notebook>=4.2->sparkmagic) (0.17.1)\nRequirement already satisfied: nbclassic>=0.4.7 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.2->sparkmagic) (1.0.0)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.0.0,>=0.17.1->sparkmagic) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.0.0,>=0.17.1->sparkmagic) (2023.3)\nRequirement already satisfied: cryptography>=1.3 in /opt/conda/lib/python3.10/site-packages (from requests_kerberos>=0.8.0->sparkmagic) (41.0.3)\nCollecting pyspnego[kerberos] (from requests_kerberos>=0.8.0->sparkmagic)\n  Obtaining dependency information for pyspnego[kerberos] from https://files.pythonhosted.org/packages/cc/fd/06a7618de50ad13b7e85115bd1e42c1625e3365313a4c971898386781f89/pyspnego-0.10.2-py3-none-any.whl.metadata\n  Downloading pyspnego-0.10.2-py3-none-any.whl.metadata (5.4 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->sparkmagic) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->sparkmagic) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->sparkmagic) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->sparkmagic) (2023.11.17)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=1.3->requests_kerberos>=0.8.0->sparkmagic) (1.15.1)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=4.0.2->sparkmagic) (0.8.3)\nRequirement already satisfied: qtconsole in /opt/conda/lib/python3.10/site-packages (from jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (5.5.1)\nRequirement already satisfied: jupyter-console in /opt/conda/lib/python3.10/site-packages (from jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (6.6.3)\nRequirement already satisfied: entrypoints in /opt/conda/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.2.2->sparkmagic) (0.4)\nRequirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.2.2->sparkmagic) (4.1.0)\nRequirement already satisfied: jupyter-server>=1.8 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (2.12.1)\nRequirement already satisfied: notebook-shim>=0.2.3 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (0.2.3)\nRequirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (0.8.4)\nRequirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (0.2.2)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (6.0.0)\nRequirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (1.5.0)\nRequirement already satisfied: testpath in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (0.6.0)\nRequirement already satisfied: defusedxml in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (0.7.1)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (4.12.2)\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (0.5.13)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (2.1.3)\nRequirement already satisfied: fastjsonschema in /opt/conda/lib/python3.10/site-packages (from nbformat->notebook>=4.2->sparkmagic) (2.18.0)\nRequirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.10/site-packages (from nbformat->notebook>=4.2->sparkmagic) (4.19.0)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=4.0.2->sparkmagic) (0.7.0)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly>=3->autovizwidget>=0.6->sparkmagic) (8.2.3)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=4.0.2->sparkmagic) (0.2.6)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas<2.0.0,>=0.17.1->sparkmagic) (1.16.0)\nRequirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.10/site-packages (from argon2-cffi->notebook>=4.2->sparkmagic) (21.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->ipykernel>=4.2.2->sparkmagic) (3.0.9)\nCollecting gssapi>=1.6.0 (from pyspnego[kerberos]->requests_kerberos>=0.8.0->sparkmagic)\n  Downloading gssapi-1.8.3.tar.gz (94 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.2/94.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting krb5>=0.3.0 (from pyspnego[kerberos]->requests_kerberos>=0.8.0->sparkmagic)\n  Downloading krb5-0.5.1.tar.gz (221 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.1/221.1 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.2->sparkmagic) (1.2.0)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.2->sparkmagic) (2.2.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.2->sparkmagic) (0.2.2)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=1.3->requests_kerberos>=0.8.0->sparkmagic) (2.21)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (23.1.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (2023.7.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (0.30.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (0.9.2)\nRequirement already satisfied: anyio>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (3.7.1)\nRequirement already satisfied: jupyter-events>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (0.9.0)\nRequirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (0.4.4)\nRequirement already satisfied: overrides in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (6.5.0)\nRequirement already satisfied: websocket-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (1.6.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.2->sparkmagic) (2.3.2.post1)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->nbconvert>=5->notebook>=4.2->sparkmagic) (0.5.1)\nRequirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from qtconsole->jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (2.4.1)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (1.1.3)\nRequirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (2.0.7)\nRequirement already satisfied: pyyaml>=5.3 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (6.0.1)\nRequirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (0.1.4)\nRequirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (0.1.1)\nRequirement already satisfied: fqdn in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (1.5.1)\nRequirement already satisfied: isoduration in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (20.11.0)\nRequirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (2.0)\nRequirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (1.3.0)\nRequirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (1.13)\nRequirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from isoduration->jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (1.2.3)\nDownloading pyspnego-0.10.2-py3-none-any.whl (129 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: sparkmagic, autovizwidget, hdijupyterutils, gssapi, krb5\n  Building wheel for sparkmagic (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sparkmagic: filename=sparkmagic-0.21.0-py3-none-any.whl size=67571 sha256=509eae7e992ba271b247888fb15ed38dfe4611209afedafe4d8c5bb06db79a5e\n  Stored in directory: /root/.cache/pip/wheels/cf/e6/43/f1918a8a9679b065cdf7580107213e21346ea7b3e929230a73\n  Building wheel for autovizwidget (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for autovizwidget: filename=autovizwidget-0.21.0-py3-none-any.whl size=14670 sha256=e51ed1bd63ecff37f9fe41c07eac7a704a001d6b624879d979716e14a988f74e\n  Stored in directory: /root/.cache/pip/wheels/89/a4/8b/8031ab10b516319e98fd9a555a49bd8ae3d3de4c8584ba4626\n  Building wheel for hdijupyterutils (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for hdijupyterutils: filename=hdijupyterutils-0.21.0-py3-none-any.whl size=7636 sha256=3e1cf978ba4e3050d656cf606f39f16155f54ad67deeb0d35a79a59a9edb8d62\n  Stored in directory: /root/.cache/pip/wheels/6d/c5/34/5ac56467aa04154f6c148ddd19b9e7771951b2d56009c3108c\n  Building wheel for gssapi (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gssapi: filename=gssapi-1.8.3-cp310-cp310-linux_x86_64.whl size=1018460 sha256=021729c64f01aa6d0285c856a733eef404d2de3506bc5e715efd6f92099384e7\n  Stored in directory: /root/.cache/pip/wheels/87/3d/a4/a49f352c16a790928eaa1c84e8a8b505f85651bf450de80761\n  Building wheel for krb5 (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for krb5: filename=krb5-0.5.1-cp310-cp310-linux_x86_64.whl size=1110410 sha256=1226fe706d0cdb9fefc8dd39db83c86ffe9e19854ceae38c3c307643bb63b237\n  Stored in directory: /root/.cache/pip/wheels/ce/b7/7c/76519a566acc1920166954bde7c5bff8dd872e8f545e417860\nSuccessfully built sparkmagic autovizwidget hdijupyterutils gssapi krb5\nInstalling collected packages: krb5, gssapi, pandas, pyspnego, requests_kerberos, jupyter, hdijupyterutils, autovizwidget, sparkmagic\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.0.3\n    Uninstalling pandas-2.0.3:\n      Successfully uninstalled pandas-2.0.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nbeatrix-jupyterlab 2023.814.150030 requires jupyter-server~=1.16, but you have jupyter-server 2.12.1 which is incompatible.\nbeatrix-jupyterlab 2023.814.150030 requires jupyterlab~=3.4, but you have jupyterlab 4.0.5 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\nfitter 1.6.0 requires pandas<3.0.0,>=2.0.3, but you have pandas 1.5.3 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflowjs 4.14.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed autovizwidget-0.21.0 gssapi-1.8.3 hdijupyterutils-0.21.0 jupyter-1.0.0 krb5-0.5.1 pandas-1.5.3 pyspnego-0.10.2 requests_kerberos-0.14.0 sparkmagic-0.21.0\nCollecting pyspark\n  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=1f178704ef66f02739c05f6ddef49629979544f13dc9b0be1b2af9cd16002c37\n  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pydantic==1.8.1","metadata":{"execution":{"iopub.status.busy":"2023-12-29T10:22:06.230512Z","iopub.execute_input":"2023-12-29T10:22:06.230903Z","iopub.status.idle":"2023-12-29T10:22:18.568670Z","shell.execute_reply.started":"2023-12-29T10:22:06.230870Z","shell.execute_reply":"2023-12-29T10:22:18.567422Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting pydantic==1.8.1\n  Downloading pydantic-1.8.1-py3-none-any.whl (125 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from pydantic==1.8.1) (4.9.0)\nInstalling collected packages: pydantic\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.5.2\n    Uninstalling pydantic-2.5.2:\n      Successfully uninstalled pydantic-2.5.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nconfection 0.1.4 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, but you have pydantic 1.8.1 which is incompatible.\nfastapi 0.101.1 requires pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4, but you have pydantic 1.8.1 which is incompatible.\nopenai 1.6.1 requires pydantic<3,>=1.9.0, but you have pydantic 1.8.1 which is incompatible.\nspacy 3.7.2 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, but you have pydantic 1.8.1 which is incompatible.\nthinc 8.2.1 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, but you have pydantic 1.8.1 which is incompatible.\nweasel 0.3.4 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, but you have pydantic 1.8.1 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pydantic-1.8.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install openai==1.6.1 httpcore==1.0.2 httpx==0.26.0 typing-extensions==4.9.0","metadata":{"execution":{"iopub.status.busy":"2023-12-29T10:19:17.345507Z","iopub.execute_input":"2023-12-29T10:19:17.345984Z","iopub.status.idle":"2023-12-29T10:19:29.003056Z","shell.execute_reply.started":"2023-12-29T10:19:17.345943Z","shell.execute_reply":"2023-12-29T10:19:29.002087Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: openai==1.6.1 in /opt/conda/lib/python3.10/site-packages (1.6.1)\nRequirement already satisfied: httpcore==1.0.2 in /opt/conda/lib/python3.10/site-packages (1.0.2)\nRequirement already satisfied: httpx==0.26.0 in /opt/conda/lib/python3.10/site-packages (0.26.0)\nRequirement already satisfied: typing-extensions==4.9.0 in /opt/conda/lib/python3.10/site-packages (4.9.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai==1.6.1) (3.7.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai==1.6.1) (1.8.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai==1.6.1) (1.10.12)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai==1.6.1) (1.3.0)\nRequirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai==1.6.1) (4.66.1)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpcore==1.0.2) (2023.11.17)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.0.2) (0.14.0)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx==0.26.0) (3.4)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.6.1) (1.1.3)\n","output_type":"stream"}]},{"cell_type":"code","source":"cache_dir = \"./cache\"","metadata":{"execution":{"iopub.status.busy":"2023-12-29T10:14:27.844335Z","iopub.execute_input":"2023-12-29T10:14:27.844671Z","iopub.status.idle":"2023-12-29T10:14:27.849305Z","shell.execute_reply.started":"2023-12-29T10:14:27.844641Z","shell.execute_reply":"2023-12-29T10:14:27.848341Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_column', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_seq_items', None)\npd.set_option('display.max_colwidth', 500)\npd.set_option('expand_frame_repr', True)","metadata":{"execution":{"iopub.status.busy":"2023-12-29T10:14:27.850463Z","iopub.execute_input":"2023-12-29T10:14:27.850726Z","iopub.status.idle":"2023-12-29T10:14:28.210793Z","shell.execute_reply.started":"2023-12-29T10:14:27.850703Z","shell.execute_reply":"2023-12-29T10:14:28.209878Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# <b>2 <span style='color:#78D118'>|</span> Setting up your Weaviate</b>\n\n[Weaviate](https://weaviate.io/) is an open-source persistent and fault-tolerant [vector database](https://weaviate.io/developers/weaviate/concepts/storage). It integrates with a variety of tools, including OpenAI and Hugging Face Transformers. You can refer to their [documentation here](https://weaviate.io/developers/weaviate/quickstart).\n\nBefore we could proceed, you need your own Weaviate Network. To start your own network, visit the [homepage](https://weaviate.io/). \n\nStep 1: Click on `Start Free` \n\n<img src=\"https://files.training.databricks.com/images/weaviate_homepage.png\" width=500>\n\nStep 2: You will be brought to this [Console page](https://console.weaviate.cloud/). If this is your first time using Weaviate, click `Register here` and pass in your credentials.\n\n<img src=\"https://files.training.databricks.com/images/weaviate_register.png\" width=500>\n\nStep 3: Click on `Create cluster` and select `Free sandbox`. Provide your cluster name. For simplicity, we will toggle `enable authentication` to be `No`. Then, hit `Create`. \n\n<img src=\"https://files.training.databricks.com/images/weaviate_create_cluster.png\" width=900>\n\nStep 4: Click on `Details` and copy the `Cluster URL` and paste in the cell below.\n","metadata":{}},{"cell_type":"markdown","source":"We will use embeddings from OpenAI,  so we will need a token from OpenAI API\n\nSteps:\n1. You need to [create an account](https://platform.openai.com/signup) on OpenAI. \n2. Generate an OpenAI [API key here](https://platform.openai.com/account/api-keys). \n\nNote: OpenAI does not have a free option, but it gives you 5€ as credit. Once you have exhausted your 5€ credit, you will need to add your payment method. You will be [charged per token usage](https://openai.com/pricing). **IMPORTANT**: It's crucial that you keep your OpenAI API key to yourself. If others have access to your OpenAI key, they will be able to charge their usage to your account! \n","metadata":{}},{"cell_type":"code","source":"import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"<FILL IN>\"\nos.environ[\"WEAVIATE_NETWORK\"] = \"<FILL IN>\"","metadata":{"execution":{"iopub.status.busy":"2023-12-29T10:22:35.903118Z","iopub.execute_input":"2023-12-29T10:22:35.903550Z","iopub.status.idle":"2023-12-29T10:22:35.908218Z","shell.execute_reply.started":"2023-12-29T10:22:35.903511Z","shell.execute_reply":"2023-12-29T10:22:35.907277Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import openai\n\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\nweaviate_network = os.environ[\"WEAVIATE_NETWORK\"]","metadata":{"execution":{"iopub.status.busy":"2023-12-29T10:22:36.998626Z","iopub.execute_input":"2023-12-29T10:22:36.999520Z","iopub.status.idle":"2023-12-29T10:22:37.004082Z","shell.execute_reply.started":"2023-12-29T10:22:36.999482Z","shell.execute_reply":"2023-12-29T10:22:37.002916Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import weaviate\n\nclient = weaviate.Client(\n    weaviate_network, additional_headers={\"X-OpenAI-Api-Key\": openai.api_key}\n)\nclient.is_ready()","metadata":{"execution":{"iopub.status.busy":"2023-12-29T10:28:04.874888Z","iopub.execute_input":"2023-12-29T10:28:04.875204Z","iopub.status.idle":"2023-12-29T10:28:05.207142Z","shell.execute_reply.started":"2023-12-29T10:28:04.875174Z","shell.execute_reply":"2023-12-29T10:28:05.205909Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"# <b>3 <span style='color:#78D118'>|</span> Spark setup</b>","metadata":{}},{"cell_type":"markdown","source":"#### Dataset\n\n\nIn this section, we are going to use the data on <a href=\"https://newscatcherapi.com/\" target=\"_blank\">news topics collected by the NewsCatcher team</a>, who collects and indexes news articles and release them to the open-source community. The dataset can be downloaded from <a href=\"https://www.kaggle.com/kotartemiy/topic-labeled-news-dataset\" target=\"_blank\">Kaggle</a>.\n","metadata":{}},{"cell_type":"code","source":"import pyspark.sql.functions as F\nfrom pyspark.sql import SparkSession\n\n# Spark in local mode else using S3\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\ndf = (\n    spark\n    .read\n    .option(\"header\", True)\n    .option(\"sep\", \";\")\n    .format(\"csv\")\n    .load(\n        f\"/kaggle/input/topic-labeled-news-dataset/labelled_newscatcher_dataset.csv\".replace(\n            \"/dbfs\", \"dbfs:\"\n        )\n    )\n)\nprint(\"DataFrame Type:\")\ndisplay(df)\nprint(\"\\n\")\nprint(\"DataFrame Contents:\")\ndisplay(df.show(10))","metadata":{"execution":{"iopub.status.busy":"2023-12-29T10:30:51.654993Z","iopub.execute_input":"2023-12-29T10:30:51.655859Z","iopub.status.idle":"2023-12-29T10:31:02.780992Z","shell.execute_reply.started":"2023-12-29T10:30:51.655825Z","shell.execute_reply":"2023-12-29T10:31:02.779917Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n23/12/29 10:30:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"},{"name":"stdout","text":"DataFrame Type:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"DataFrame[topic: string, link: string, domain: string, published_date: string, title: string, lang: string]"},"metadata":{}},{"name":"stdout","text":"\n\nDataFrame Contents:\n+-------+--------------------+--------------------+-------------------+--------------------+----+\n|  topic|                link|              domain|     published_date|               title|lang|\n+-------+--------------------+--------------------+-------------------+--------------------+----+\n|SCIENCE|https://www.eurek...|      eurekalert.org|2020-08-06 13:59:45|A closer look at ...|  en|\n|SCIENCE|https://www.pulse...|            pulse.ng|2020-08-12 15:14:19|An irresistible s...|  en|\n|SCIENCE|https://www.expre...|       express.co.uk|2020-08-13 21:01:00|Artificial intell...|  en|\n|SCIENCE|https://www.ndtv....|            ndtv.com|2020-08-03 22:18:26|Glaciers Could Ha...|  en|\n|SCIENCE|https://www.thesu...|           thesun.ie|2020-08-12 19:54:36|Perseid meteor sh...|  en|\n|SCIENCE|https://interesti...|interestingengine...|2020-08-08 11:05:45|NASA Releases In-...|  en|\n|SCIENCE|https://www.thequ...|        thequint.com|2020-05-28 09:09:46|SpaceX, NASA Demo...|  en|\n|SCIENCE|https://www.thesp...|  thespacereview.com|2020-08-10 22:48:23|Orbital space tou...|  en|\n|SCIENCE|https://www.busin...| businessinsider.com|2020-08-16 00:28:54|Greenland's melti...|  en|\n|SCIENCE|https://www.thehi...|thehindubusinessl...|2020-08-14 07:43:25|NASA invites engi...|  en|\n+-------+--------------------+--------------------+-------------------+--------------------+----+\nonly showing top 10 rows\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}}]},{"cell_type":"markdown","source":"# <b>4 <span style='color:#78D118'>|</span> Dataset into Weaviate</b>","metadata":{}},{"cell_type":"markdown","source":"We are going to store this dataset in the Weaviate database. To do that, we first need to define a schema. A schema is where we define classes, class properties, data types, and vectorizer modules we would like to use. \n\nIn the schema below, notice that:\n\n- We capitalize the first letter of `class_name`. This is Weaviate's rule. \n- We specify data types within `properties`\n- We use `text2vec-openai` as the vectorizer. \n- You can also choose to upload your own vectors (refer to [docs here](https://weaviate.io/developers/weaviate/api/rest/objects#with-a-custom-vector)) or create a class without any vectors (but we won't be able to perform similarity search after).\n\n[Reference documentation here](https://weaviate.io/developers/weaviate/tutorials/schema)\n","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Schema","metadata":{}},{"cell_type":"code","source":"class_name = \"News\"\nclass_obj = {\n    \"class\": class_name,\n    \"description\": \"News topics collected by NewsCatcher\",\n    \"properties\": [\n        {\"name\": \"topic\", \"dataType\": [\"string\"]},\n        {\"name\": \"link\", \"dataType\": [\"string\"]},\n        {\"name\": \"domain\", \"dataType\": [\"string\"]},\n        {\"name\": \"published_date\", \"dataType\": [\"string\"]},\n        {\"name\": \"title\", \"dataType\": [\"string\"]},\n        {\"name\": \"lang\", \"dataType\": [\"string\"]},\n    ],\n    \"vectorizer\": \"text2vec-openai\",\n}\n","metadata":{"execution":{"iopub.status.busy":"2023-12-29T10:34:48.560502Z","iopub.execute_input":"2023-12-29T10:34:48.561462Z","iopub.status.idle":"2023-12-29T10:34:48.569066Z","shell.execute_reply.started":"2023-12-29T10:34:48.561419Z","shell.execute_reply":"2023-12-29T10:34:48.568151Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# If the class exists before, we will delete it first\nif client.schema.exists(class_name):\n    print(\"Deleting existing class...\")\n    client.schema.delete_class(class_name)\n\nprint(f\"Creating class: '{class_name}'\")\nclient.schema.create_class(class_obj)","metadata":{"execution":{"iopub.status.busy":"2023-12-29T10:35:03.934830Z","iopub.execute_input":"2023-12-29T10:35:03.935280Z","iopub.status.idle":"2023-12-29T10:35:03.977165Z","shell.execute_reply.started":"2023-12-29T10:35:03.935245Z","shell.execute_reply":"2023-12-29T10:35:03.976256Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Creating class: 'News'\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\nprint(json.dumps(client.schema.get(class_name), indent=4))","metadata":{"execution":{"iopub.status.busy":"2023-12-29T10:35:25.908155Z","iopub.execute_input":"2023-12-29T10:35:25.908514Z","iopub.status.idle":"2023-12-29T10:35:25.921229Z","shell.execute_reply.started":"2023-12-29T10:35:25.908487Z","shell.execute_reply":"2023-12-29T10:35:25.920281Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"{\n    \"class\": \"News\",\n    \"description\": \"News topics collected by NewsCatcher\",\n    \"invertedIndexConfig\": {\n        \"bm25\": {\n            \"b\": 0.75,\n            \"k1\": 1.2\n        },\n        \"cleanupIntervalSeconds\": 60,\n        \"stopwords\": {\n            \"additions\": null,\n            \"preset\": \"en\",\n            \"removals\": null\n        }\n    },\n    \"moduleConfig\": {\n        \"text2vec-openai\": {\n            \"baseURL\": \"https://api.openai.com\",\n            \"model\": \"ada\",\n            \"modelVersion\": \"002\",\n            \"type\": \"text\",\n            \"vectorizeClassName\": true\n        }\n    },\n    \"multiTenancyConfig\": {\n        \"enabled\": false\n    },\n    \"properties\": [\n        {\n            \"dataType\": [\n                \"text\"\n            ],\n            \"indexFilterable\": true,\n            \"indexSearchable\": true,\n            \"moduleConfig\": {\n                \"text2vec-openai\": {\n                    \"skip\": false,\n                    \"vectorizePropertyName\": false\n                }\n            },\n            \"name\": \"topic\",\n            \"tokenization\": \"whitespace\"\n        },\n        {\n            \"dataType\": [\n                \"text\"\n            ],\n            \"indexFilterable\": true,\n            \"indexSearchable\": true,\n            \"moduleConfig\": {\n                \"text2vec-openai\": {\n                    \"skip\": false,\n                    \"vectorizePropertyName\": false\n                }\n            },\n            \"name\": \"link\",\n            \"tokenization\": \"whitespace\"\n        },\n        {\n            \"dataType\": [\n                \"text\"\n            ],\n            \"indexFilterable\": true,\n            \"indexSearchable\": true,\n            \"moduleConfig\": {\n                \"text2vec-openai\": {\n                    \"skip\": false,\n                    \"vectorizePropertyName\": false\n                }\n            },\n            \"name\": \"domain\",\n            \"tokenization\": \"whitespace\"\n        },\n        {\n            \"dataType\": [\n                \"text\"\n            ],\n            \"indexFilterable\": true,\n            \"indexSearchable\": true,\n            \"moduleConfig\": {\n                \"text2vec-openai\": {\n                    \"skip\": false,\n                    \"vectorizePropertyName\": false\n                }\n            },\n            \"name\": \"published_date\",\n            \"tokenization\": \"whitespace\"\n        },\n        {\n            \"dataType\": [\n                \"text\"\n            ],\n            \"indexFilterable\": true,\n            \"indexSearchable\": true,\n            \"moduleConfig\": {\n                \"text2vec-openai\": {\n                    \"skip\": false,\n                    \"vectorizePropertyName\": false\n                }\n            },\n            \"name\": \"title\",\n            \"tokenization\": \"whitespace\"\n        },\n        {\n            \"dataType\": [\n                \"text\"\n            ],\n            \"indexFilterable\": true,\n            \"indexSearchable\": true,\n            \"moduleConfig\": {\n                \"text2vec-openai\": {\n                    \"skip\": false,\n                    \"vectorizePropertyName\": false\n                }\n            },\n            \"name\": \"lang\",\n            \"tokenization\": \"whitespace\"\n        }\n    ],\n    \"replicationConfig\": {\n        \"factor\": 1\n    },\n    \"shardingConfig\": {\n        \"virtualPerPhysical\": 128,\n        \"desiredCount\": 1,\n        \"actualCount\": 1,\n        \"desiredVirtualCount\": 128,\n        \"actualVirtualCount\": 128,\n        \"key\": \"_id\",\n        \"strategy\": \"hash\",\n        \"function\": \"murmur3\"\n    },\n    \"vectorIndexConfig\": {\n        \"skip\": false,\n        \"cleanupIntervalSeconds\": 300,\n        \"maxConnections\": 64,\n        \"efConstruction\": 128,\n        \"ef\": -1,\n        \"dynamicEfMin\": 100,\n        \"dynamicEfMax\": 500,\n        \"dynamicEfFactor\": 8,\n        \"vectorCacheMaxObjects\": 1000000000000,\n        \"flatSearchCutoff\": 40000,\n        \"distance\": \"cosine\",\n        \"pq\": {\n            \"enabled\": false,\n            \"bitCompression\": false,\n            \"segments\": 0,\n            \"centroids\": 256,\n            \"trainingLimit\": 100000,\n            \"encoder\": {\n                \"type\": \"kmeans\",\n                \"distribution\": \"log-normal\"\n            }\n        }\n    },\n    \"vectorIndexType\": \"hnsw\",\n    \"vectorizer\": \"text2vec-openai\"\n}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Step 2: Save\n\nNow that the class is created, we are going to write our dataframe to the class. \n\n**IMPORTANT!!** Since we are writing a Spark DataFrame out, we need a Spark Connector to Weaviate. You need to [download the Spark connector jar file](https://github.com/weaviate/spark-connector#download-jar-from-github) and [upload to your Databricks cluster](https://github.com/weaviate/spark-connector#using-the-jar-in-databricks) before running the next cell. If you do not do this, the next cell *will fail*.\n","metadata":{}},{"cell_type":"code","source":"(\n    df.limit(100)\n    .write.format(\"io.weaviate.spark.Weaviate\")\n    .option(\"scheme\", \"http\")\n    .option(\"host\", weaviate_network.split(\"https://\")[1])\n    .option(\"header:X-OpenAI-Api-Key\", openai.api_key)\n    .option(\"className\", class_name)\n    .mode(\"append\")\n    .save()\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check if the data is indeed populated. You can run either the following command or go to \n`https://{insert_your_cluster_url_here}/v1/objects` \n\nYou should be able to see the data records, rather than null objects.\n","metadata":{}},{"cell_type":"code","source":"client.query.get(\"News\", [\"topic\"]).do()","metadata":{"execution":{"iopub.status.busy":"2023-12-28T21:37:26.455483Z","iopub.execute_input":"2023-12-28T21:37:26.456101Z","iopub.status.idle":"2023-12-28T21:37:26.464835Z","shell.execute_reply.started":"2023-12-28T21:37:26.456066Z","shell.execute_reply":"2023-12-28T21:37:26.462456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like the data is populated. We can proceed further and do a query search. We are going to search for any news titles related to `locusts`. Additionally, we are going to add a filter statement, where the topic of the news has to be `SCIENCE`. Notice that we don't have to carry out the step of converting `locusts` into embeddings ourselves because we have included a vectorizer within the class earlier on.\n\nWe will use `with_near_text` to specify the text we would like to query similar titles for. By default, Weaviate uses cosine distance to determine similar objects. Refer to [distance documentation here](https://weaviate.io/developers/weaviate/config-refs/distances#available-distance-metrics).\n","metadata":{"execution":{"iopub.status.busy":"2023-12-28T12:53:52.039407Z","iopub.execute_input":"2023-12-28T12:53:52.039784Z","iopub.status.idle":"2023-12-28T12:53:52.047634Z","shell.execute_reply.started":"2023-12-28T12:53:52.039754Z","shell.execute_reply":"2023-12-28T12:53:52.046524Z"}}},{"cell_type":"code","source":"where_filter = {\n    \"path\": [\"topic\"],\n    \"operator\": \"Equal\",\n    \"valueString\": \"SCIENCE\",\n}\n\n# We are going to search for any titles related to locusts\nnear_text = {\"concepts\": \"locust\"}\n(\n    client.query.get(class_name, [\"topic\", \"domain\", \"title\"])\n    .with_where(where_filter)\n    .with_near_text(near_text)\n    .with_limit(2)\n    .do()\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-28T21:38:37.008394Z","iopub.execute_input":"2023-12-28T21:38:37.008939Z","iopub.status.idle":"2023-12-28T21:38:43.020085Z","shell.execute_reply.started":"2023-12-28T21:38:37.008897Z","shell.execute_reply":"2023-12-28T21:38:43.018939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alternatively, if you wish to supply your own embeddings at query time, you can do that too. Since embeddings are vectors, we will use `with_near_vector` instead.\n\nIn the code block below, we additionally introduce a `distance` parameter. The lower the distance score is, the closer the vectors are to each other. Read more about the distance thresholds [here](https://weaviate.io/developers/weaviate/config-refs/distances#available-distance-metrics).\n","metadata":{}},{"cell_type":"code","source":"import openai\n\nmodel = \"text-embedding-ada-002\"\nopenai_object = openai.Embedding.create(input=[\"locusts\"], model=model)\n\nopenai_embedding = openai_object[\"data\"][0][\"embedding\"]\n\n(\n    client.query.get(\"News\", [\"topic\", \"domain\", \"title\"])\n    .with_where(where_filter)\n    .with_near_vector(\n        {\n            \"vector\": openai_embedding,\n            \"distance\": 0.7,  # this sets a threshold for distance metric\n        }\n    )\n    .with_limit(2)\n    .do()\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-28T21:38:48.540384Z","iopub.execute_input":"2023-12-28T21:38:48.542293Z","iopub.status.idle":"2023-12-28T21:38:48.961869Z","shell.execute_reply.started":"2023-12-28T21:38:48.542222Z","shell.execute_reply":"2023-12-28T21:38:48.960845Z"},"trusted":true},"execution_count":null,"outputs":[]}]}